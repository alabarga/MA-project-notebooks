{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Finding geo-names in the Flydubai Leak documents\n",
    "## *An evaluation of GUI software for text analysis*\n",
    "\n",
    "\n",
    "On July 29, The Guardian published a [report](https://www.theguardian.com/business/2016/jul/29/airline-pilots-complain-dangerous-fatigue-leaked-documents-flydubai) about internal complains from the staff of Flydubai, a government-owned airline from the UAE.\n",
    "\n",
    "The report was based on [documents](https://www.theguardian.com/business/2016/jul/29/flydubai-flight-records-the-leaked-documents) leaked to that newspaper, and that were also published on the same date.\n",
    "\n",
    "![](https://s19.postimg.org/s7zv1g2n7/flydubai_cover.png)\n",
    "\n",
    "From our project's perspective, the content of the documents looked very interesting for a possible practice with geographical names, which abounded in the document.\n",
    "\n",
    "But instead of treating this corpus as any other one, we decided to start trying some of the tools that are available in the market for journalists not working with programming for text mining.\n",
    "\n",
    "The idea was to see how effective these tools can be, and possibly compare results.\n",
    "\n",
    "### Description of the content\n",
    "\n",
    "Period covered by events in the complains: March and April 2016\n",
    "\n",
    "Guardian's note on spelling: *The misspellings are as they appear in the documents; English is the language used by pilots, but it is not necessarily a pilot’s mother tongue.*\n",
    "\n",
    "**Guardian's findings**: In all, the reports include:\n",
    "\n",
    "- 42 complaints about or experience of fatigue; \n",
    "- 25 bird strikes; \n",
    "- 10 medical emergencies; \n",
    "- 5 laser incidents; one bomb threat; \n",
    "- 1 “dogs on the runway”; \n",
    "- 1 unstable aircraft due to unstable truffles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Collecting the data\n",
    "\n",
    "Data collection in this case was very simple: we copied and pasted the text that was all in one web page (we tried to find copies of the original documents but the *Guardian* did not release them). We saved the info to a utf-8 text file.\n",
    "\n",
    "## (2) Using text mining software for content analysis\n",
    "\n",
    "## DocumentCloud\n",
    "\n",
    "We first uploaded the file to [DocumentCloud](www.documentcloud.org), a project the IRE (Investigative Reporters and Editors) has sponsored since 2011, which not only is a \"catalog of primary source documents,\" it also works as \"a tool for annotating, organizing and publishing\" documents on the web.\n",
    "\n",
    "![](https://s19.postimg.org/v2gsk48yr/flydubai_doccloud.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the options in the buttons are about the document(s)' meta data, not about its content, except for the drop down menu `Analyze`. \n",
    "\n",
    "We used the Entity analysis feature and these is what we found:\n",
    "\n",
    "![](https://s19.postimg.org/vqytujmg3/results_doccloud.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the many mentions of the one person found is because he is the photographer in the piece we are analysing (we didn't do any manual data cleaning on purpose, to deal with dirty data with the resources offered by these programs).\n",
    "\n",
    "Another option in this menu allows for the extraction of dates. This document includes only one:\n",
    "\n",
    "![](https://s19.postimg.org/5y40vaulf/flydubai_timeline_doccloud.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the kinds of native entity extraction performed by DocumentCloud. Note that, as indicated in their website, this analysis includes running the documents uploaded through Thomson Reuters' [OpenCalais](http://www.opencalais.com), so this test is also indicative of the capabilities of that other tool.\n",
    "\n",
    "A third option in the `Analysis` menu is analysing the document(s) using OverviewDocs.\n",
    "\n",
    "## OverviewDocs\n",
    "\n",
    "Overview is also a text analysis platform created with journalists in mind: \"Overview began at The Associated Press, supported by the John S. and James L. Knight Foundation as part of its Knight News Challenge,\" reads the *About* section in their website.\n",
    "\n",
    "The platform has way more functionalities than DocumentCloud, and can be used separately, without having to have an account in DocumentCloud (that restricts its services to registered news organisations). In fact, separate registration is a requirement for DocumentCloud users wanting to use Overview.\n",
    "\n",
    "The tool offers: \"built-in OCR, a sophisticated search engine, word clouds, entity detection, and topic-based document clustering. It has sophisticated tagging and metadata support and supports many input and export formats. If you need custom analysis, you can write your own plugins using the API.\"\n",
    "\n",
    "The first thing we see when we open our document in Overview is a word cloud and a series of tabs:\n",
    "\n",
    "![](https://s19.postimg.org/6c5cuweoz/word_cloud_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Multisearch` tab we can search for a term and get the number of documents containing it (in this case we only have one document, so that is not particularly useful in this case). If we activate the search within documents to the right, we also get a preview of the matches highlighted in colour:\n",
    "\n",
    "![](https://s19.postimg.org/5b545rxpf/overview_multisearch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-name extraction\n",
    "\n",
    "We finally get to our original motivation as journalists in the search for a story: the geographical names mentioned in the document.\n",
    "\n",
    "In its `Entities` tab, Overview offers a series of options, one of which is the extraction of country names, based, as you can see in the image, on a list of names provided by [this website](http://www.geonames.org/countries/):\n",
    "\n",
    "![](https://s19.postimg.org/qm79ns57n/geo_names_countries_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding results manually\n",
    "\n",
    "Once we have the list, the program gives us the option to manually exclude inaccurate results. We can use the document search to the right to make sure that the word in question is not the name of a place. It's unfortunate that search options don't include the possibility to search for exact words:\n",
    "\n",
    "![](https://s19.postimg.org/hfoz0hzz7/excluding_names_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We excluded \"go to\", \"pain\", \"some\", \"a man\", \"men\", \"sat\", \"bi\", \"end\" and \"cat\".\n",
    "\n",
    "A big problem with this feature is that we didn't find a way to export those results. \n",
    "\n",
    "The experience with the names of cities was different, rendering a wider range of results, which in this case is a negative thing, the filter is not as good as it should be (we have excluded the list of stop words from the results):\n",
    "\n",
    "![](https://s19.postimg.org/tvlou8tb7/geocities.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were accomplished when we excluded the Google Books words, but again, it was not possible to export or copy the list of results. As the website states in its About section, it was created with investigative journalists in mind, so this may be the reason why the pipelining for data journalists to continue to do their analysis is not very well implemented.\n",
    "\n",
    "## (3) Processing the document with Python libraries\n",
    "\n",
    "Having seen these options, we proceed to analyse our document with the tools programming (NLTK and other Python libraries in this case) has to offer.\n",
    "\n",
    "We begin importing the libraries we need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and opening/reading our file to start working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open(\"FlyDubai leak documents.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) Preliminary data exploration\n",
    "\n",
    "It's always useful to get an idea of the extension of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88746"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) Finding geo-names based on the intersections of sets of words\n",
    "\n",
    "We are going to start applying the same method used by WordCloud and Overview for entity extraction: comparing a document with a given set of words that identify something we want to discover in the text.\n",
    "\n",
    "In this case, we are going to compare our text with a known list of geographical names.\n",
    "\n",
    "Let's start cleaning/normalising the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nltk, pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# we separate the text into its tokens\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# transforming the text into lower case and storing it in a different variable\n",
    "lowercase_text = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of those cases in which keeping a copy of the text with the original capitalisation can be helpful, given that geographical names could be easier to find that way.\n",
    "\n",
    "We are going to use the same list used by Overview, available [here](http://www.geonames.org/countries/).\n",
    "\n",
    "We transferred the list to a csv file, now we can import it here and assign its content to a data frame named `countries_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO-3166</th>\n",
       "      <th>ISO-3166.1</th>\n",
       "      <th>fips</th>\n",
       "      <th>Country</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD</td>\n",
       "      <td>AND</td>\n",
       "      <td>AN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>ARE</td>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ISO-3166 ISO-3166.1 fips               Country           Capital Continent\n",
       "0       AD        AND   AN               Andorra  Andorra la Vella        EU\n",
       "1       AE        ARE   AE  United Arab Emirates         Abu Dhabi        AS"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df = pd.read_csv(\"geonames.csv\", encoding = \"ISO-8859-1\")\n",
    "countries_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now turn some of its columns into lists, that we can use as filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_name = countries_df[\"Country\"].tolist()\n",
    "city_name = countries_df[\"Capital\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a very simple function to extract the matches from our document and store the results in a list that we have called `countries_flydubai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_flydubai = []\n",
    "for w in tokens:\n",
    "    if w in country_name:\n",
    "        if w not in countries_flydubai:\n",
    "            countries_flydubai.append(w)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Armenia',\n",
       " 'Azerbaijan',\n",
       " 'Bahrain',\n",
       " 'Bangladesh',\n",
       " 'Egypt',\n",
       " 'Georgia',\n",
       " 'India',\n",
       " 'Iran',\n",
       " 'Iraq',\n",
       " 'Kazakhstan',\n",
       " 'Kuwait',\n",
       " 'Lebanon',\n",
       " 'Nepal',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Qatar',\n",
       " 'Russia',\n",
       " 'Slovakia',\n",
       " 'Somalia',\n",
       " 'Sudan',\n",
       " 'Tajikistan',\n",
       " 'Ukraine']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we sort the list to visualise the results in alphabetical order\n",
    "sorted(countries_flydubai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for the cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities_flydubai = []\n",
    "for w in tokens:\n",
    "    if w in city_name:\n",
    "        if w not in cities_flydubai:\n",
    "            cities_flydubai.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Astana',\n",
       " 'Baghdad',\n",
       " 'Baku',\n",
       " 'Beirut',\n",
       " 'Bratislava',\n",
       " 'Brussels',\n",
       " 'Colombo',\n",
       " 'Dhaka',\n",
       " 'Doha',\n",
       " 'Dushanbe',\n",
       " 'Juba',\n",
       " 'Kabul',\n",
       " 'Kathmandu',\n",
       " 'Khartoum',\n",
       " 'Kiev',\n",
       " 'Moscow',\n",
       " 'Muscat',\n",
       " 'Riyadh',\n",
       " 'Sarajevo',\n",
       " 'Tbilisi',\n",
       " 'Tehran']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cities_flydubai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to notice the deficiencies of our list/filter: Dubai is not on the list because it is not the capital of the UAE. We need to work on the creation of better quality filters if we want to use this method.\n",
    "\n",
    "We can now get an idea of the number of elements in each search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of cities mentioned in the document\n",
    "len(cities_flydubai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of countries mentioned in the document\n",
    "len(countries_flydubai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting the search to the characteristics of the document\n",
    "\n",
    "The advantage of being able to write code for text analysis is that we can adapt calculations at will, something that can be especially useful with documents that have very specific characteristics. \n",
    "\n",
    "When working with text data, it is good practice to do an exploration of the text (skim through it) to get familiar with its structure, and try to detect any special features that can facilitate its analysis.\n",
    "\n",
    "When we did that, we found the following pattern:\n",
    "\n",
    "![](https://s19.postimg.org/qidg7izqb/doc_flydubai_copia.jpg)\n",
    "\n",
    "Most of the complains were identified with the airport codes connected in each flight.\n",
    "\n",
    "Let's work with that.\n",
    "\n",
    "### Using regular expression for entity extraction\n",
    "\n",
    "When we realised the connection codes responded to the structure XXX-XXX and XXXX-XXXX (that is, groups of three and four capital letters), we decided to use regular expressions to extract them from the text.\n",
    "\n",
    "We begin by importing the `re` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LYP-DXB',\n",
       " 'URKK-OMBD',\n",
       " 'DXB-JED',\n",
       " 'DXB-KBL',\n",
       " 'DXB-KBL',\n",
       " 'DXB-KHI',\n",
       " 'BTS-DXB',\n",
       " 'DXB-KWI',\n",
       " 'AAA-AAA']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# We use this website https://regex101.com to write our expressions\n",
    "\n",
    "# Going for capital letters only [A-Z] will eliminate results of hyphenated terms, such as \"turn-around\", and year ranges\n",
    "# like \"2015-2016\"\n",
    "\n",
    "p = re.compile('([A-Z]{3,4}-[A-Z]{3,4})')\n",
    "p.findall(text)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a sample of the results, the 10 first results to be exact. Now we can calculate totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of matches\n",
    "flights = p.findall(text)\n",
    "len(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique number of matches (using `set`)\n",
    "len(set(flights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to get more specific results counting the repetitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DXB-KHI', 5),\n",
       " ('DXB-KBL', 3),\n",
       " ('DXB-TIF', 3),\n",
       " ('MCT-DXB', 2),\n",
       " ('BTS-DXB', 2),\n",
       " ('DXB-JED', 2),\n",
       " ('KDH-DXB', 2),\n",
       " ('DXB-BGW', 2),\n",
       " ('DXB-KWI', 2),\n",
       " ('IEV-DXB', 2),\n",
       " ('DXB-TSE', 2),\n",
       " ('URKK-OMBD', 2),\n",
       " ('DXB-SJJ', 1),\n",
       " ('DXB-JUB', 1),\n",
       " ('OIKB-OMDB', 1),\n",
       " ('OMDB-URKK', 1),\n",
       " ('DXB-HBE', 1),\n",
       " ('AJAK-GIDO', 1),\n",
       " ('DXB-DYU', 1),\n",
       " ('MUX-DXB', 1),\n",
       " ('DXB-ELQ', 1),\n",
       " ('DXB-MED', 1),\n",
       " ('DXB-KRT', 1),\n",
       " ('KRT-DXB', 1),\n",
       " ('ALA-DXB', 1),\n",
       " ('ADER-OGOG', 1),\n",
       " ('HGA-DXB', 1),\n",
       " ('VKO-DXB', 1),\n",
       " ('DXB-AHB', 1),\n",
       " ('LYP-DXB', 1),\n",
       " ('MHD-DXB', 1),\n",
       " ('DWC-DXB', 1),\n",
       " ('DYU-DXB', 1),\n",
       " ('DXB-OAI', 1),\n",
       " ('DXB-GYD', 1),\n",
       " ('KWI-DXB', 1),\n",
       " ('DXB-TBS', 1),\n",
       " ('DXB-BEY', 1),\n",
       " ('OMDB-OAKB', 1),\n",
       " ('DAC-DXB', 1),\n",
       " ('KHI-DXB', 1),\n",
       " ('DXB-SKT', 1),\n",
       " ('ODS-DXB', 1),\n",
       " ('DXB-COK', 1),\n",
       " ('CMB-DXB', 1),\n",
       " ('DXB-TRV', 1),\n",
       " ('KWI-DWC', 1),\n",
       " ('DXB-IEV', 1),\n",
       " ('DXB-DAC', 1),\n",
       " ('AAA-AAA', 1),\n",
       " ('LKO-KTM', 1),\n",
       " ('VCBI-OMDB', 1),\n",
       " ('RUH-DXB', 1)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "\n",
    "from collections import Counter\n",
    "countofFlights = Counter(flights)\n",
    "\n",
    "# http://stackoverflow.com/questions/20950650/how-to-sort-counter-by-value-python\n",
    "\n",
    "countofFlights.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above is the full list of results, which is already a good start in terms of results (we could use that to create visualisation of the airports connected and which itineraries presented the most complains.\n",
    "\n",
    "We can transform the data even more. Let's transform the codes into readable names of cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to convert our list of flights to text (string) to be able to replace the text for new text\n",
    "flights_as_string = \",\".join(str(x) for x in flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dubai-Kiev,LYP-Dubai,URKK-OMBD,Dubai-Jeddah,Dubai-Kabul,Dubai-Kabul,Dubai-Karachi,Bratislava-Dubai,Dubai-Kuwait,AAA-AAA,OMDB-URKK,Muscat-Dubai,RUH-Dubai,Dubai-Taif,Dubai-HBE,Bratislava-Dubai,Dubai-TSE,Dubai-Kabul,Dubai-MED,MUX-Dubai,Dubai-BGW,Dubai-TRV,Dubai-GYD,Dubai-SJJ,Dubai-Karachi,Dubai-AHB,Dubai-DAC,KRT-Dubai,Kuwait-DWC,Dubai-COK,Dubai-TSE,CMB-Dubai,Dubai-TBS,Kiev-Dubai,Kiev-Dubai,Dubai-KRT,Dubai-Karachi,Kuwait-Dubai,Dubai-Karachi,Dubai-Karachi,Dubai-Taif,ALA-Dubai,LKO-KTM,Kandahar-Dubai,Karachi-Dubai,Dubai-BEY,DAC-Dubai,DYU-Dubai,Kandahar-Dubai,Dubai-DYU,HGA-Dubai,VCBI-OMDB,Dubai-Jeddah,URKK-OMBD,OIKB-OMDB,Dubai-JUB,Dubai-ELQ,Muscat-Dubai,DWC-Dubai,Dubai-BGW,Dubai-Kuwait,VKO-Dubai,OMDB-OAKB,ADER-OGOG,AJAK-GIDO,MHD-Dubai,Dubai-SKT,ODS-Dubai,Dubai-Taif,Dubai-OAI'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_as_string = flights_as_string.replace(\"DXB\", \"Dubai\")\n",
    "flights_as_string = flights_as_string.replace(\"IEV\", \"Kiev\")\n",
    "flights_as_string = flights_as_string.replace(\"KHI\", \"Karachi\")\n",
    "flights_as_string = flights_as_string.replace(\"KBL\", \"Kabul\")\n",
    "flights_as_string = flights_as_string.replace(\"TIF\", \"Taif\")\n",
    "flights_as_string = flights_as_string.replace(\"MCT\", \"Muscat\")\n",
    "flights_as_string = flights_as_string.replace(\"BTS\", \"Bratislava\")\n",
    "flights_as_string = flights_as_string.replace(\"JED\", \"Jeddah\")\n",
    "flights_as_string = flights_as_string.replace(\"KWI\", \"Kuwait\")\n",
    "flights_as_string = flights_as_string.replace(\"KDH\", \"Kandahar\")\n",
    "flights_as_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now repeat the search, the same as above, to visualise the count with the city names changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dubai-Karachi', 5),\n",
       " ('Dubai-Kabul', 3),\n",
       " ('Dubai-Taif', 3),\n",
       " ('Bratislava-Dubai', 2),\n",
       " ('Kiev-Dubai', 2),\n",
       " ('Dubai-Jeddah', 2),\n",
       " ('Kandahar-Dubai', 2),\n",
       " ('Muscat-Dubai', 2),\n",
       " ('URKK-OMBD', 2),\n",
       " ('Dubai-TSE', 2),\n",
       " ('Dubai-Kuwait', 2),\n",
       " ('Dubai-BGW', 2),\n",
       " ('Dubai-SJJ', 1),\n",
       " ('Dubai-TRV', 1),\n",
       " ('OIKB-OMDB', 1)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = re.compile('(\\w*-\\w*)')\n",
    "flights_with_names = p2.findall(flights_as_string)\n",
    "Counter(flights_with_names).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed attempt\n",
    "\n",
    "We know that the search and replace process used above could be automated, but we didn't get the code to run. We have preserved our fail attempts below, because we may be able to fix the code in the future, but we think our algorithm to solve the problem is right (we just need to figure out how to translate it to code).\n",
    "\n",
    "This is what we want to do:\n",
    "\n",
    "- We created a filter based on [this list]() of international airport codes\n",
    "- We created a tuples based on the combination of (City, Airport Code)\n",
    "- We tried to use the replace method above, providing the \"Airport Code\" as the \"old\" argument, and the \"City\" as the \"new\" argument\n",
    "- We tried to use that idea in a for loop, but tuples are not iterable, according to the result we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# airports_df = pd.read_csv(\"international airport codes.csv\", encoding = \"ISO-8859-1\")\n",
    "# airports_df.head(2)\n",
    "\n",
    "# airports_df['pairs'] = airports_df[[\"Airport Code\", \"City\"]].apply(tuple, axis=1)\n",
    "# airports_df['pairs'][1][1]\n",
    "\n",
    "# code = airports_df['pairs'][10][i][0] <---- we tried to create an iteration and used it as the index for the tuple\n",
    "                                            # but that didn't work\n",
    "# city = airports_df['pairs'][10][i][1]\n",
    "# flights_as_string.replace(code, city)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# origin_dubai = re.compile(\"(DXB-.*)\")   <---- We also tried this option in the for loop below, but we didn´t get it to work\n",
    "# destination = re.compile(\"DXB-(.*)\")\n",
    "\n",
    "# for w in flights_as_string:\n",
    "    # if w==origin_dubai:\n",
    "        # readable_list.append(\"Dubai-\" + destination)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "The information we have so far is enough to create a visualisation of the flights and the most affected connections. But it could also be the starting point for an investigation into the worst connections in terms of complaints. Here a few possible questions:\n",
    "\n",
    "- explore correlations between flight duration and complains\n",
    "- explore correlations between most problematic connections and known air traffic accidents\n",
    "- intercept incidents with weather conditions\n",
    "\n",
    "![](https://s19.postimg.org/6zdgbwqcj/flydubai_map.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Guardian (2016), *Airline pilots complain of dangerous fatigue in leaked documents* https://www.theguardian.com/business/2016/jul/29/airline-pilots-complain-dangerous-fatigue-leaked-documents-flydubai\n",
    "\n",
    "- Guardian (2016), *Flydubai flight records – the leaked documents* https://www.theguardian.com/business/2016/jul/29/flydubai-flight-records-the-leaked-documents \n",
    "\n",
    "- NLTK 3.0 Documentation (2015), *Corpus readers*, available at http://www.nltk.org/howto/corpus.html#corpus-reader-classes\n",
    "\n",
    "- PythonHow (2016) *Accessing pandas dataframe columns, rows, and cells* http://pythonhow.com/accessing-dataframe-columns-rows-and-cells\n",
    "\n",
    "- Stack Overflow (2010) *How can I count the occurrences of a list item in Python?* http://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item-in-python\n",
    "\n",
    "- Stack Overflow (2011) *Python count elements in list [duplicate]* http://stackoverflow.com/questions/4130027/python-count-elements-in-list\n",
    "\n",
    "- Stack Overflow (2011) *Reference an Element in a List of Tuples* http://stackoverflow.com/questions/6454894/reference-an-element-in-a-list-of-tuples\n",
    "\n",
    "- Stack Overflow (2013) *How to form tuple column from two columns in Pandas* http://stackoverflow.com/questions/16031056/how-to-form-tuple-column-from-two-columns-in-pandas\n",
    "\n",
    "- Stack Overflow (2014) *Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries* http://stackoverflow.com/questions/23317342/pandas-dataframe-split-column-into-multiple-columns-right-align-inconsistent-c\n",
    "\n",
    "- Stack Overflow (2014) *Creating a pandas DataFrame from columns of other DataFrames with similar indexes* http://stackoverflow.com/questions/21231834/creating-a-pandas-dataframe-from-columns-of-other-dataframes-with-similar-indexe\n",
    "\n",
    "- TutorialsPoint (2016) *Python List count() Method* http://www.tutorialspoint.com/python/list_count.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
